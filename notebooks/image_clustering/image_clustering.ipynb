{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70270770-8428-4bd1-badb-173cf8a2be5b",
   "metadata": {},
   "source": [
    "# Clustering de imagenes y b√∫squeda de imagenes\n",
    "\n",
    "INCOMPLETO.\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "- [Image Clustering Implementation with PyTorch](https://towardsdatascience.com/image-clustering-implementation-with-pytorch-587af1d14123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea92504-d666-4328-a51e-08ede7e83671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9c1b1-ad73-4771-a980-bbc3fdfd7fa7",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9264ff0f-3519-47bf-ae8b-9e62124b36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG(nn.Module):\n",
    "    '''Encoder of image based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "\n",
    "        self.encoder = self._encodify_(vgg)\n",
    "        \n",
    "    def _encodify_(self, encoder):\n",
    "        '''Create list of modules for encoder based on the architecture in VGG template model.\n",
    "        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling\n",
    "        indices from the corresponding pooling operation in the encoder. In VGG template, these indices\n",
    "        are not returned. Hence the need for this method to extent the pooling operations.\n",
    "        Args:\n",
    "            encoder : the template VGG model\n",
    "        Returns:\n",
    "            modules : the list of modules that define the encoder corresponding to the VGG model\n",
    "        '''\n",
    "        modules = nn.ModuleList()\n",
    "        for module in encoder.features:\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,\n",
    "                                          stride=module.stride,\n",
    "                                          padding=module.padding,\n",
    "                                          return_indices=True)\n",
    "                modules.append(module_add)\n",
    "            else:\n",
    "                modules.append(module)\n",
    "\n",
    "        return modules\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): code tensor\n",
    "            pool_indices (list): Pool indices tensors in order of the pooling modules\n",
    "        '''\n",
    "        pool_indices = []\n",
    "        x_current = x\n",
    "        for module_encode in self.encoder:\n",
    "            output = module_encode(x_current)\n",
    "\n",
    "            # If the module is pooling, there are two outputs, the second the pool indices\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                x_current = output[0]\n",
    "                pool_indices.append(output[1])\n",
    "            else:\n",
    "                x_current = output\n",
    "\n",
    "        return x_current, pool_indices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd03e6-c2a2-4d7f-9627-09e5da382d39",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efabb3ba-63e7-4a99-aefa-00bdf60e8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(nn.Module):\n",
    "    '''Decoder of code based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_code\n",
    "    channels_out = 3\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "\n",
    "        self.decoder = self._invert_(encoder)\n",
    "        \n",
    "    def _invert_(self, encoder):\n",
    "        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder\n",
    "        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose\n",
    "        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder\n",
    "        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization\n",
    "        and activation, and a maxpooling in encoder is turned into unpooling.\n",
    "        Args:\n",
    "            encoder (ModuleList): the encoder\n",
    "        Returns:\n",
    "            decoder (ModuleList): the decoder obtained by \"inversion\" of encoder\n",
    "        '''\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,\n",
    "                          'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.ConvTranspose2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "\n",
    "        # Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "\n",
    "        return nn.ModuleList(modules_transpose)\n",
    "    \n",
    "    def forward(self, x, pool_indices):\n",
    "        '''Execute the decoder on the code tensor input\n",
    "        Args:\n",
    "            x (Tensor): code tensor obtained from encoder\n",
    "            pool_indices (list): Pool indices Pytorch tensors in order the pooling modules in the encoder\n",
    "        Returns:\n",
    "            x (Tensor): decoded image tensor\n",
    "        '''\n",
    "        x_current = x\n",
    "\n",
    "        k_pool = 0\n",
    "        reversed_pool_indices = list(reversed(pool_indices))\n",
    "        for module_decode in self.decoder:\n",
    "\n",
    "            # If the module is unpooling, collect the appropriate pooling indices\n",
    "            if isinstance(module_decode, nn.MaxUnpool2d):\n",
    "                x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])\n",
    "                k_pool += 1\n",
    "            else:\n",
    "                x_current = module_decode(x_current)\n",
    "\n",
    "        return x_current   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823dcbe2-d0ee-4f51-8545-45fad9650469",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611a430-ac32-41f9-8a5d-b168763e623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderVGG(nn.Module):\n",
    "    '''Auto-Encoder based on the VGG-16 with batch normalization template model. The class is comprised of\n",
    "    an encoder and a decoder.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_code = EncoderVGG.channels_code\n",
    "    channels_out = DecoderVGG.channels_out\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(AutoEncoderVGG, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderVGG(pretrained_params=pretrained_params)\n",
    "        self.decoder = DecoderVGG(self.encoder.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward the autoencoder for image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_prime (Tensor): image tensor following encoding and decoding\n",
    "        '''\n",
    "        code, pool_indices = self.encoder(x)\n",
    "        x_prime = self.decoder(code, pool_indices)\n",
    "\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5f4d7-f4e6-4c9d-9e1e-98e438b57bcd",
   "metadata": {},
   "source": [
    "## Local aggregation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cca897-49d2-4795-829b-ff2fa7dbe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "\n",
    "class LocalAggregationLoss(nn.Module):\n",
    "    '''Local Aggregation Loss module from \"Local Aggregation for Unsupervised Learning of Visual Embeddings\" by\n",
    "    Zhuang, Zhai and Yamins (2019), arXiv:1903.12355v2\n",
    "    '''\n",
    "    def __init__(self, temperature,\n",
    "                 k_nearest_neighbours, clustering_repeats, number_of_centroids,\n",
    "                 memory_bank,\n",
    "                 kmeans_n_init=1, nn_metric=cosine_distance, nn_metric_params={}):\n",
    "        super(LocalAggregationLoss, self).__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.memory_bank = memory_bank\n",
    "\n",
    "        self.neighbour_finder = NearestNeighbors(n_neighbors=k_nearest_neighbours + 1,\n",
    "                                                 algorithm='ball_tree',\n",
    "                                                 metric=nn_metric, metric_params=nn_metric_params)\n",
    "        self.clusterer = []\n",
    "        for k_clusterer in range(clustering_repeats):\n",
    "            self.clusterer.append(KMeans(n_clusters=number_of_centroids,\n",
    "                                         init='random', n_init=kmeans_n_init))\n",
    "            \n",
    "    def forward(self, codes, indices):\n",
    "        '''Forward pass for the local aggregation loss module'''\n",
    "        assert codes.shape[0] == len(indices)\n",
    "\n",
    "        codes = codes.type(torch.DoubleTensor)\n",
    "        code_data = normalize(codes.detach().numpy(), axis=1)\n",
    "\n",
    "        # Compute and collect arrays of indices that define the constants in the loss function. Note that\n",
    "        # no gradients are computed for these data values in backward pass\n",
    "        self.memory_bank.update_memory(code_data, indices)\n",
    "        \n",
    "        background_neighbours = self._nearest_neighbours(code_data, indices)\n",
    "        close_neighbours = self._close_grouper(indices)\n",
    "        neighbour_intersect = self._intersecter(background_neighbours, close_neighbours)\n",
    "\n",
    "        # Compute the probability density for the codes given the constants of the memory bank\n",
    "        v = F.normalize(codes, p=2, dim=1)\n",
    "        d1 = self._prob_density(v, background_neighbours)\n",
    "        d2 = self._prob_density(v, neighbour_intersect)\n",
    "        \n",
    "        return torch.sum(torch.log(d1) - torch.log(d2)) / codes.shape[0]            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756d4f0-860d-4848-95e6-99cf0c9c6ff1",
   "metadata": {},
   "source": [
    "## Memory bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b2c0a-1460-480f-876e-18a58f7e8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(object):\n",
    "    '''Memory bank\n",
    "    Args:\n",
    "        n_vectors (int): Number of vectors the memory bank should hold\n",
    "        dim_vector (int): Dimension of the vectors the memory bank should hold\n",
    "        memory_mixing_rate (float, optional): Fraction of new vector to add to currently stored vector. The value\n",
    "            should be between 0.0 and 1.0, the greater the value the more rapid the update. The mixing rate can be\n",
    "            set during calling `update_memory`.\n",
    "    '''\n",
    "    def __init__(self, n_vectors, dim_vector, memory_mixing_rate):\n",
    "\n",
    "        self.dim_vector = dim_vector\n",
    "        self.vectors = np.array([marsaglia(dim_vector) for _ in range(n_vectors)])\n",
    "        self.memory_mixing_rate = memory_mixing_rate\n",
    "        self.mask_init = np.array([False] * n_vectors)\n",
    "\n",
    "    def update_memory(self, vectors, index):\n",
    "        '''Update the memory with new vectors'''\n",
    "        if isinstance(index, int):\n",
    "            self.vectors[index] = self._update_(vectors, self.vectors[index])\n",
    "\n",
    "        elif isinstance(index, np.ndarray):\n",
    "            for ind, vector in zip(index, vectors):\n",
    "                self.vectors[ind] = self._update_(vector, self.vectors[ind])\n",
    "\n",
    "    def mask(self, inds_int):\n",
    "        '''Construct a Boolean mask given integer indices'''\n",
    "        ret_mask = []\n",
    "        for row in inds_int:\n",
    "            row_mask = np.full(self.vectors.shape[0], False)\n",
    "            row_mask[row.astype(int)] = True\n",
    "            ret_mask.append(row_mask)\n",
    "\n",
    "        return np.array(ret_mask)\n",
    "\n",
    "    def _update_(self, vector_new, vector_recall):\n",
    "        return vector_new * self.memory_mixing_rate + vector_recall * (1.0 - self.memory_mixing_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
