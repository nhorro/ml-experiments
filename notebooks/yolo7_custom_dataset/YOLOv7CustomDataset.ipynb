{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74e0ddd-7bf5-4a9e-ad87-491fbca3c7a7",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelo detector con YOLOv7 (PyTorch)\n",
    "\n",
    "Este cuaderno contiene los pasos para entrenar un modelo YOLOv7 en PyTorch, exportar para inferencia con Triton Server e integrar en una cadena de [videoanalytics](https://github.com/nhorro/videoanalytics).\n",
    "\n",
    "**Entradas**:\n",
    "\n",
    "- Dataset para detección de objetos con YOLOv7.\n",
    "\n",
    "**Salidas**:\n",
    "\n",
    "- Modelo YOLOv7 entrenado en PyTorch (`.pt`). \n",
    "- Modelo exportado a TensorRT para inferencia en Triton Server.\n",
    "\n",
    "**Resumen del procedimiento**\n",
    "\n",
    "1. Descarga de YOLOv7 e instalación de dependencias.\n",
    "2. Descarga y preparación de dataset.\n",
    "3. Entrenamiento.\n",
    "4. Evaluación.\n",
    "5. Inferencia para prototipado rápido (PyTorch).\n",
    "6. Despliegue con Triton Server.\n",
    "7. Integración en videoanalytics.\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "- [Official YOLOv7](https://github.com/WongKinYiu/yolov7)\n",
    "- [Fine Tuning YOLOv7 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/)\n",
    "- Datasets:\n",
    "    - https://public.roboflow.com/object-detection/pothole/1\n",
    "    - https://www.researchgate.net/publication/282807920_Dataset_of_images_used_for_pothole_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50b8c3-2994-40c8-a0cb-3113649eedae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Descarga de YOLOv7 e instalación de dependencias\n",
    "\n",
    "Este cuaderno asume que se ejecuta en un ambiente donde YOLOv7 Aún no ha sido instalado. \n",
    "\n",
    "Se descargará en el subdirectorio `yolov7`. La mayorìa de los pasos que siguen se ejecutan adentro del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6105b-c473-4e8e-ac97-4e2fa2867f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('yolov7'):\n",
    "    !git clone https://github.com/WongKinYiu/yolov7.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d24d7-716e-407b-b6cf-04b2c0465023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ace-419c-444b-bfaf-7cc5c7897b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac1419-2980-4f5b-b979-9683ffc76f27",
   "metadata": {},
   "source": [
    "## 2. Descarga y preparación de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a9464-74e6-4b49-94fd-c06644aaafd8",
   "metadata": {},
   "source": [
    "### Dataset de ejemplo: Pothole\n",
    "\n",
    "Fuente: [Fine Tuning YOLOv7 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132f4d2-9449-48a7-99d3-d2a4f954f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8096da2-8172-44cc-91f7-4219f77bdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('pothole_dataset.zip'):\n",
    "    !wget https://learnopencv.s3.us-west-2.amazonaws.com/pothole_dataset.zip\n",
    "    !unzip -q pothole_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d03023-48fc-4b8d-914c-561133ced0f5",
   "metadata": {},
   "source": [
    "Estructura de directorios de dataset de entrada.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ae430-f1b8-4345-a927-dd7d6543d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -d pothole_dataset | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123937eb-7a41-4cc6-8ca1-91422c722088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pothole_dataset/images/train | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ecd3c-a3c5-4e4e-ba85-a40b415797a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pothole_dataset/labels/train | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6a4a0-976a-4336-91f6-911a4ac4afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat pothole_dataset/labels/train/G0010033.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73646176-ac16-434c-b7a3-175783dc9801",
   "metadata": {},
   "source": [
    "Formato: \n",
    "\n",
    "~~~\n",
    "class, x_center, y_center, width, height\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf336719-a2ec-4ea7-9ca0-c79f9f3c76d0",
   "metadata": {},
   "source": [
    "### Definición del dataset\n",
    "\n",
    "Debe ir en `yolov7/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bf84c-d30c-433a-9ae4-4beaf2bc8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b906c-1d86-43d5-a6b4-49cdebb68fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/pothole.yaml\n",
    "train: ../pothole_dataset/images/train\n",
    "val: ../pothole_dataset/images/valid\n",
    "test: ../pothole_dataset/images/test\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: ['pothole']  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35724ac6-aa0e-46f2-946c-39f9f4689a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/pothole.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3a835-b958-4573-9bfe-43531418e243",
   "metadata": {},
   "source": [
    "## 2. Configuración del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f23c7-4806-4207-92f1-4a8eef127300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff5f53-b48e-4604-b1ae-1e76b6b38bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls cfg/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b258c-26fb-411d-816f-93091a17667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cfg/training/yolov7.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb31a6-f0a5-46b2-8949-ab9fdc0e5b50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Atención</b>: editar manualmente este archivo luego de copiar. Cómo mínimo, se debe establecer el número de clases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391ad8e-c0d7-4b01-a8a9-4d7f0f1cd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp cfg/training/yolov7.yaml cfg/training/yolov7-pothole.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9641f-ba21-40e9-87b4-da45e3f8a438",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf901101-22af-47f5-bec7-6f968097434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716704b-11ae-47dc-b90e-0150e1f89f7b",
   "metadata": {},
   "source": [
    "Descargar pesos iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc005d1-fa81-4b22-b6fb-caab33c8d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('yolov7_training.pt'): \n",
    "    !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9e187-ff5e-467b-aeb4-c7f86919b5f7",
   "metadata": {},
   "source": [
    "Verificar disponibilidad de GPU(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0c1bd-46d1-463d-bdbc-eb150e7e2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213a3a-711a-449d-903a-35e57e03b93a",
   "metadata": {},
   "source": [
    "Si se dispone de recursos se puede aumentar el número de workers y tamaño de batch. En una laptop con RTX3070MQ funcionó sólo con `workers=1` y `batch-size=4`.\n",
    "\n",
    "Se puede monitorear el entrenamiento con Tensorboard.\n",
    "\n",
    "En una terminal aparte:\n",
    "\n",
    "~~~bash\n",
    "tensorboard --logdir runs/train\n",
    "~~~\n",
    "\n",
    "Tensorboard:  http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0cb6f-9ee1-4979-b260-7d46c200cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --epochs 100 \\\n",
    "                 --workers 1 \\\n",
    "                 --device 0 \\\n",
    "                 --batch-size 4 \\\n",
    "                 --data data/pothole.yaml \\\n",
    "                 --img 640 640 \\\n",
    "                 --cfg cfg/training/yolov7_pothole.yaml \\\n",
    "                 --weights 'yolov7_training.pt' \\\n",
    "                 --name yolov7_pothole \\\n",
    "                 --hyp data/hyp.scratch.custom.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa7bf1-b316-44e0-8d1b-50e77c9505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/train/yolov7_pothole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96042a89-130c-49d6-90dd-2f0581dd7bca",
   "metadata": {},
   "source": [
    "## 4. Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85900a0b-d96a-484b-87c9-7ce565594e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --data data/pothole.yaml \\\n",
    "               --img 640 \\\n",
    "               --batch 32 \\\n",
    "               --conf 0.001 \\\n",
    "               --iou 0.65 \\\n",
    "               --device 0 \\\n",
    "               --weights runs/train/yolov7_pothole8/weights/best.pt \\\n",
    "               --name yolov7_640_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e384c6-61aa-42f2-83b6-af4c7c12ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/test/yolov7_640_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b799c2-0a02-4667-af88-da9d57f896ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='runs/test/yolov7_640_val/confusion_matrix.png',width=600,height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c57000-8cd5-4cf2-a5af-cad8312fde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='runs/test/yolov7_640_val/test_batch0_labels.jpg',width=1024,height=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b543c4-26cb-4a70-844b-9db8082b295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='runs/test/yolov7_640_val/test_batch1_labels.jpg',width=1024,height=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5596faf-067a-4788-92ed-cd0a7d6fe58d",
   "metadata": {},
   "source": [
    "## 5. Inferencia para prototipado rápido (PyTorch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3ff26-cb49-476c-a39d-dede623ee632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De https://github.com/pytorch/pytorch/issues/18325: torch.load() requires model module in the same folder #3678\n",
    "%cd yolov7\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e869cc7-eef0-4dab-820a-33e8824b2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/train/yolov7_pothole8/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa965b5c-7102-4474-a0e7-5e80fcebbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Atención: torch.load() requires model module in the same folder #3678\n",
    "MODEL_WEIGHTS_PATH=\"runs/train/yolov7_pothole8/weights/best.pt\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weigths = torch.load(MODEL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f5239-febb-4458-ab11-4fce75a1b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = weigths['model']\n",
    "model = model.half().to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a9bc0-58b6-4d92-a2df-c4b5cbe2f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "TEST_IMG_PATH='pothole_dataset/images/test/img-294_jpg.rf.a16953e9091e3eecfc338ed3044ef294.jpg'\n",
    "img = cv2.imread(TEST_IMG_PATH) \n",
    "img =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e08031-6492-40d8-8c65-81b112ec8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img.copy()\n",
    "image = transforms.ToTensor()(image)\n",
    "image = torch.tensor(np.array([image.numpy()]))\n",
    "image = image.to(device)\n",
    "image = image.half()\n",
    "with torch.no_grad():\n",
    "    output, _ = model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a1fc8-e235-43cd-8f8b-4a5c91c15149",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Despliegue con Triton Server\n",
    "\n",
    "Se siguen los pasos del [github oficial](https://github.com/WongKinYiu/yolov7/tree/main/deploy/triton-inference-server).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d49d64-b988-4e0b-8663-118a4b06f913",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Advertencia sobre compatibilidad entre TensorRT y CUDA**\n",
    "\n",
    "Elegir la versión de TensorRT que se corresponda con la versión de CUDA disponible en el host. De lo contrario aparecen errores.\n",
    "\n",
    "0. Determinar la versión de CUDA en el sistema.\n",
    "\n",
    "~~~bash\n",
    "nvidia-smi\n",
    "~~~\n",
    "\n",
    "1. Exportar a ONNX. \n",
    "\n",
    "Nota: instalar antes todas las dependencias:\n",
    "\n",
    "~~~bash\n",
    "pip install onnx onnx-simplifier onnx-graphsurgeon\n",
    "~~~\n",
    "\n",
    "~~~bash\n",
    "cd yolov7\n",
    "python export.py --weights runs/train/yolov7_pothole8/weights/best.pt --grid --end2end --dynamic-batch --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640\n",
    "~~~\n",
    "\n",
    "2. Exportar a TensorRT con docker de Triton. Elegir el que corresponda, por ejemplo, para CUDA 11.6 es 22.02.\n",
    "\n",
    "~~~bash\n",
    "docker run -it --rm --gpus=all nvcr.io/nvidia/tensorrt:22.02-py3\n",
    "~~~\n",
    "\n",
    "3. Copiar al docker.\n",
    "\n",
    "~~~bash\n",
    "docker ps\n",
    "CONTAINER ID   IMAGE                               COMMAND                  CREATED         STATUS         PORTS     NAMES\n",
    "14c431abcf03   nvcr.io/nvidia/tensorrt:22.06-py3   \"/opt/nvidia/nvidia_…\"   2 minutes ago   Up 2 minutes             dreamy_northcutt\n",
    "~~~\n",
    "\n",
    "~~~bash\n",
    "cd yolov7\n",
    "docker cp runs/train/yolov7_pothole8/weights/best.onnx dreamy_northcutt:/workspace/\n",
    "~~~\n",
    "\n",
    "4. Convertir a TensorRT\n",
    "\n",
    "~~~bash\n",
    "mv best.onnx yolov7.onnx\n",
    "./tensorrt/bin/trtexec --onnx=yolov7.onnx --minShapes=images:1x3x640x640 --optShapes=images:8x3x640x640 --maxShapes=images:8x3x640x640 --fp16 --workspace=4096 --saveEngine=yolov7-fp16-1x8x8.engine --timingCacheFile=timing.cache\n",
    "~~~\n",
    "\n",
    "5. Servicio con Triton\n",
    "\n",
    "Crear estructura de directorios:\n",
    "\n",
    "~~~bash\n",
    "cd yolo7_custom_dataset\n",
    "mv yolov7/yolov7-fp16-1x8x8.engine triton-deploy/models/yolov7/1/model.plan\n",
    "mkdir -pv triton-deploy/models/yolov7/1\n",
    "touch triton-deploy/models/yolov7/config.pbtxt\n",
    "~~~\n",
    "\n",
    "Editar `config.pbtxt`:\n",
    "\n",
    "~~~\n",
    "name: \"yolov7\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 8\n",
    "dynamic_batching { }\n",
    "~~~\n",
    "\n",
    "**Advertencia**: elegir la versión de Triton Server, se debe usar la misma versión de TensorRT que se usó para exportar. Ver [matriz de compatibilidad](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html)\n",
    "\n",
    "~~~bash\n",
    "docker run --gpus all --rm --ipc=host --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/models:/models nvcr.io/nvidia/tritonserver:22.02-py3 tritonserver --model-repository=/models --strict-model-config=false --log-verbose 1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fe6fc-9b97-41f8-94a8-0873e1995f18",
   "metadata": {},
   "source": [
    "## 7. Integración en Videoanalytics\n",
    "\n",
    "En esta sección se asume que se ejecuta la cadena de videoanalytics en el ambiente de desarrollo con PyTorch, que ya tiene opencv, numpy, y otras dependencias.\n",
    "\n",
    "Consideraciones:\n",
    "\n",
    "- Puede haber conflictos de GPU. Si hay una única GPU, es conveniente que Triton la use. Si la cadena de videoanalyitcs tiene otros modelos que usen GPU, conviene también llevarlos a Triton.\n",
    "- Videoanalyitcs no aprovecha la capacidad de procesar por batch.\n",
    "\n",
    "Descargar videoanalyitcs. \n",
    "\n",
    "**Nota**: La versión actual de videoanalytics aún no tiene un detector de objetos con YOLOv7/Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f6e369-1aa6-466e-a934-259d5456d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"videoanalytics\"):\n",
    "    !git clone https://github.com/nhorro/videoanalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a39f7-d51f-4d9e-a71b-549bc959e75b",
   "metadata": {},
   "source": [
    "Instalar las dependencias faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b41b7af-01c1-460a-b193-5620edf6f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a821c9e-b8e4-4170-b9e3-e2b041b03c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"videoanalytics/src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d34f7-40be-4d5d-b000-d5d111f383d0",
   "metadata": {},
   "source": [
    "### 7.1 Componente de detección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d150e56-e167-47fe-a3b1-8bbb561dd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "from videoanalytics.utils.boundingboxes import format_boxes\n",
    "from videoanalytics.pipeline import Sink\n",
    "import numpy as np\n",
    "\n",
    "class YOLOv7DetectorTriton(Sink):\n",
    "    '''\n",
    "    YOLOv7 object detector Triton Client implementation.\n",
    "    This component **READS** the following entries in the global context:\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    | Variable name     | Description                                         |\n",
    "    +===================+============+==========+=============================+\n",
    "    | FRAME             | Numpy array representing the frame.                 |\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    This component **UPDATES** the following entries in the global context:\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    | Variable name     | Description                                         |\n",
    "    +===================+============+==========+=============================+\n",
    "    | DETECTIONS        | List holding numpy array with bounding boxes.       |\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    Args:\n",
    "        name(str): the component unique name.\n",
    "        context (dict): The global context.         \n",
    "        triton_server_uri (str): Triton server endpoint. \n",
    "        allowed_classes (list): set of allowed classes. This option is to restrict\n",
    "                                the detections to a subset of classes relevant to\n",
    "                                the application domain. If None, all classes are allowed. \n",
    "                                Note: Ignored in this version.\n",
    "        yolo_input_size (int): size in pixels of the input cell. The input image is \n",
    "                               resized using opencv. \n",
    "        yolo_max_output_size_per_class (int): maximum number of detections per class. \n",
    "                                              Note: Ignored in this version.\n",
    "        yolo_max_total_size (int): maximum number of detections. \n",
    "                                   Note: Ignored in this version.\n",
    "        context_name(str): variable name used for storing detections in context\n",
    "    '''    \n",
    "    def __init__(self,name,context, model_name=\"yolov7\", url='localhost:8001', context_name=\"DETECTIONS\"):\n",
    "        super().__init__(name, context)\n",
    "        \n",
    "        self.context_name=context_name\n",
    "        self.model_name=model_name\n",
    "        self.yolo_input_size=640\n",
    "        \n",
    "        self.letter_box=False # No soportado por ahora\n",
    "        \n",
    "        # Create server context        \n",
    "        self.triton_client = grpcclient.InferenceServerClient(\n",
    "            url=url,\n",
    "            verbose=False,\n",
    "            ssl=False,\n",
    "            root_certificates=None,\n",
    "            private_key=None,\n",
    "            certificate_chain=None)\n",
    "        \n",
    "        # Health check\n",
    "        assert(self.triton_client.is_server_live())\n",
    "        assert(self.triton_client.is_server_ready())\n",
    "        assert(self.triton_client.is_model_ready(self.model_name))\n",
    "        \n",
    "        self.INPUT_NAMES = [\"images\"]\n",
    "        self.OUTPUT_NAMES = [\"num_dets\", \"det_boxes\", \"det_scores\", \"det_classes\"]\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.inputs.append(grpcclient.InferInput(self.INPUT_NAMES[0], [1, 3, self.yolo_input_size, self.yolo_input_size], \"FP32\"))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[0]))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[1]))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[2]))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[3]))\n",
    "        \n",
    "    def setup(self):\n",
    "        pass\n",
    "    \n",
    "    def __preprocess(self):\n",
    "        if self.letter_box:            \n",
    "            img_h, img_w, _ = self.context[\"FRAME\"].shape\n",
    "            new_h, new_w =   self.yolo_input_size,  self.yolo_input_size \n",
    "            offset_h, offset_w = 0, 0\n",
    "            if (new_w / img_w) <= (new_h / img_h):\n",
    "                new_h = int(img_h * new_w / img_w)\n",
    "                offset_h = (self.yolo_input_size - new_h) // 2\n",
    "            else:\n",
    "                new_w = int(img_w * new_h / img_h)\n",
    "                offset_w = (self.yolo_input_size - new_w) // 2\n",
    "            resized = cv2.resize(self.context[\"FRAME\"], (new_w, new_h))\n",
    "            img = np.full((self.yolo_input_size, self.yolo_input_size, 3), 127, dtype=np.uint8)\n",
    "            img[offset_h:(offset_h + new_h), offset_w:(offset_w + new_w), :] = resized\n",
    "        else:\n",
    "            img = cv2.resize(self.context[\"FRAME\"], (self.yolo_input_size, self.yolo_input_size))\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.transpose((2, 0, 1)).astype(np.float32)\n",
    "        img /= 255.0\n",
    "        return img\n",
    "            \n",
    "    def process(self):\n",
    "        input_image_buffer = self.__preprocess()\n",
    "        input_image_buffer = np.expand_dims(input_image_buffer, axis=0)\n",
    "        \n",
    "        self.inputs[0].set_data_from_numpy(input_image_buffer)\n",
    "        results = self.triton_client.infer(model_name=self.model_name,\n",
    "                                           inputs=self.inputs,\n",
    "                                           outputs=self.outputs)\n",
    "                \n",
    "        num_objects = results.as_numpy(self.OUTPUT_NAMES[0])[0][0]\n",
    "        bboxes = results.as_numpy(self.OUTPUT_NAMES[1])[0]\n",
    "        scores = np.squeeze(results.as_numpy(self.OUTPUT_NAMES[2]))        \n",
    "        classes = np.squeeze(results.as_numpy(self.OUTPUT_NAMES[3]))        \n",
    "        bboxes = bboxes[0:int(num_objects)].copy()       \n",
    "        scores = scores[0:int(num_objects)].copy()        \n",
    "        classes = classes[0:int(num_objects)].copy()\n",
    "        \n",
    "        # 6. Convertir BBs de normalized ymin, xmin, ymax, xmax ---> xmin, ymin, width, height\n",
    "        original_h, original_w, _ = self.context[\"FRAME\"].shape\n",
    "        \n",
    "        for box in bboxes:\n",
    "            box[2]-=box[0]\n",
    "            box[3]-=box[1]\n",
    "            box[0]*=original_w/self.yolo_input_size\n",
    "            box[1]*=original_h/self.yolo_input_size\n",
    "            box[2]*=original_w/self.yolo_input_size\n",
    "            box[3]*=original_h/self.yolo_input_size\n",
    "            \n",
    "        # 7. FIXME: encontrar una forma mejor de representar las detecciones\n",
    "        self.context[self.context_name] = [bboxes, scores, classes, num_objects]\n",
    "        \n",
    "    def shutdown(self):\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f582387-f5aa-4cf2-a352-fd4f1b0a9808",
   "metadata": {},
   "source": [
    "### 7.2 Aplicación de detección\n",
    "\n",
    "- [Demo en youtube](https://www.youtube.com/shorts/o1D1467VvFo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32966916-0283-4fa5-8934-fd3bdd162bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00831294059753418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 12,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63573bd588cf47f399f3ba84ed64b7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_avg_dt': 0.0004877093459786888, 'detector_avg_dt': 0.018640747988061583, 'annotator_avg_dt': 4.77036348903519e-06, 'writer_avg_dt': 0.002689606319297781}\n"
     ]
    }
   ],
   "source": [
    "from videoanalytics.pipeline import Pipeline\n",
    "from videoanalytics.pipeline.sources import VideoReader\n",
    "from videoanalytics.pipeline.sinks import VideoWriter\n",
    "from videoanalytics.pipeline.sinks.object_detection import DetectionsAnnotator, DetectionsCSVWriter\n",
    "\n",
    "# Input\n",
    "INPUT_VIDEO = \"pothole_test/test_video/video3.mp4\"\n",
    "OUTPUT_VIDEO = \"pothole_test/output/output3.mp4\"\n",
    "START_FRAME = 0\n",
    "MAX_FRAMES = None\n",
    "\n",
    "# Classes names for Detections Annotator\n",
    "DETECTOR_CLASSES_FILENAME = \"pothole_test/classes.txt\"\n",
    "\n",
    "\n",
    "# 1. Create the context\n",
    "context = {}\n",
    "\n",
    "# 2. Create the pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# 3. Add components\n",
    "pipeline.add_component( VideoReader( \"input\",context,\n",
    "                                     video_path=INPUT_VIDEO,\n",
    "                                     start_frame=START_FRAME,\n",
    "                                     max_frames=MAX_FRAMES))\n",
    "\n",
    "# 3.2 Detector\n",
    "pipeline.add_component( YOLOv7DetectorTriton(\"detector\",context) )\n",
    "\n",
    "# 3.4 Annotate detections in output video\n",
    "pipeline.add_component( DetectionsAnnotator(\"annotator\",context,\n",
    "                                             class_names_filename=DETECTOR_CLASSES_FILENAME,\n",
    "                                             show_label=True) )\n",
    "\n",
    "pipeline.add_component(VideoWriter(\"writer\",context,filename=OUTPUT_VIDEO))\n",
    "\n",
    "# 4. Define connections\n",
    "pipeline.set_connections([\n",
    "    (\"input\", \"detector\"),\n",
    "    (\"detector\", \"annotator\"),\n",
    "    (\"annotator\", \"writer\")\n",
    "])\n",
    "                       \n",
    "# 5. Execute\n",
    "pipeline.execute()\n",
    "\n",
    "# 6. Report (optional)\n",
    "print(pipeline.get_metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
