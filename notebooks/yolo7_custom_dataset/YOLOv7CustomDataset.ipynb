{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74e0ddd-7bf5-4a9e-ad87-491fbca3c7a7",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelo detector con YOLOv7 (PyTorch)\n",
    "\n",
    "Este cuaderno contiene los pasos para entrenar un modelo YOLOv7 en PyTorch y exportar para inferencia con Triton Server.\n",
    "\n",
    "**Entradas**:\n",
    "\n",
    "- Dataset para detección de objetos con YOLOv7\n",
    "\n",
    "**Salidas**:\n",
    "\n",
    "- Modelo YOLOv7 entrenado en PyTorch (`.pt`).\n",
    "- Modelo exportado para inferencia en Triton Server.\n",
    "\n",
    "**Resumen del procedimiento**\n",
    "\n",
    "1. Descarga de YOLOv7 e instalación de dependencias.\n",
    "2. Descarga y preparación de dataset.\n",
    "3. Entrenamiento.\n",
    "4. Evaluación.\n",
    "5. Inferencia.\n",
    "6. Despliegue con Triton Server.\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "- [Official YOLOv7](https://github.com/WongKinYiu/yolov7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50b8c3-2994-40c8-a0cb-3113649eedae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Descarga de YOLOv7 e instalación de dependencias\n",
    "\n",
    "Este cuaderno asume que se ejecuta en un ambiente donde YOLOv7 Aún no ha sido instalado. \n",
    "\n",
    "Se descargará en el subdirectorio `yolov7`. La mayorìa de los pasos que siguen se ejecutan adentro del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6105b-c473-4e8e-ac97-4e2fa2867f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('yolov7'):\n",
    "    !git clone https://github.com/WongKinYiu/yolov7.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d24d7-716e-407b-b6cf-04b2c0465023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ace-419c-444b-bfaf-7cc5c7897b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac1419-2980-4f5b-b979-9683ffc76f27",
   "metadata": {},
   "source": [
    "## 2. Descarga y preparación de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a9464-74e6-4b49-94fd-c06644aaafd8",
   "metadata": {},
   "source": [
    "### Dataset de ejemplo: Pothole\n",
    "\n",
    "Fuente: [Fine Tuning YOLOv7 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132f4d2-9449-48a7-99d3-d2a4f954f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8096da2-8172-44cc-91f7-4219f77bdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('pothole_dataset.zip'):\n",
    "    !wget https://learnopencv.s3.us-west-2.amazonaws.com/pothole_dataset.zip\n",
    "    !unzip -q pothole_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d03023-48fc-4b8d-914c-561133ced0f5",
   "metadata": {},
   "source": [
    "Estructura de directorios de dataset de entrada.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ae430-f1b8-4345-a927-dd7d6543d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -d pothole_dataset | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123937eb-7a41-4cc6-8ca1-91422c722088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pothole_dataset/images/train | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ecd3c-a3c5-4e4e-ba85-a40b415797a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pothole_dataset/labels/train | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6a4a0-976a-4336-91f6-911a4ac4afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat pothole_dataset/labels/train/G0010033.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73646176-ac16-434c-b7a3-175783dc9801",
   "metadata": {},
   "source": [
    "Formato: \n",
    "\n",
    "~~~\n",
    "class, x_center, y_center, width, height\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf336719-a2ec-4ea7-9ca0-c79f9f3c76d0",
   "metadata": {},
   "source": [
    "### Definición del dataset\n",
    "\n",
    "Debe ir en `yolov7/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bf84c-d30c-433a-9ae4-4beaf2bc8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b906c-1d86-43d5-a6b4-49cdebb68fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/pothole.yaml\n",
    "train: ../pothole_dataset/images/train\n",
    "val: ../pothole_dataset/images/valid\n",
    "test: ../pothole_dataset/images/test\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: ['pothole']  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35724ac6-aa0e-46f2-946c-39f9f4689a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/pothole.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3a835-b958-4573-9bfe-43531418e243",
   "metadata": {},
   "source": [
    "## 2. Configuración del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f23c7-4806-4207-92f1-4a8eef127300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff5f53-b48e-4604-b1ae-1e76b6b38bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls cfg/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b258c-26fb-411d-816f-93091a17667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cfg/training/yolov7.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb31a6-f0a5-46b2-8949-ab9fdc0e5b50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Atención</b>: editar manualmente este archivo luego de copiar. Cómo mínimo, se debe establecer el número de clases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391ad8e-c0d7-4b01-a8a9-4d7f0f1cd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp cfg/training/yolov7.yaml cfg/training/yolov7-pothole.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9641f-ba21-40e9-87b4-da45e3f8a438",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf901101-22af-47f5-bec7-6f968097434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de estar adentro de yolov7\n",
    "cwd=%pwd\n",
    "if cwd.split('/')[-1] != 'yolov7':\n",
    "    %cwd yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716704b-11ae-47dc-b90e-0150e1f89f7b",
   "metadata": {},
   "source": [
    "Descargar pesos iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc005d1-fa81-4b22-b6fb-caab33c8d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('yolov7_training.pt'): \n",
    "    !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9e187-ff5e-467b-aeb4-c7f86919b5f7",
   "metadata": {},
   "source": [
    "Verificar disponibilidad de GPU(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0c1bd-46d1-463d-bdbc-eb150e7e2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213a3a-711a-449d-903a-35e57e03b93a",
   "metadata": {},
   "source": [
    "Si se dispone de recursos se puede aumentar el número de workers y tamaño de batch. En una laptop con RTX3070MQ funcionó sólo con `workers=1` y `batch-size=4`.\n",
    "\n",
    "Se puede monitorear el entrenamiento con Tensorboard.\n",
    "\n",
    "En una terminal aparte:\n",
    "\n",
    "~~~bash\n",
    "tensorboard --logdir runs/train\n",
    "~~~\n",
    "\n",
    "Tensorboard:  http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0cb6f-9ee1-4979-b260-7d46c200cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --epochs 100 \\\n",
    "                 --workers 1 \\\n",
    "                 --device 0 \\\n",
    "                 --batch-size 4 \\\n",
    "                 --data data/pothole.yaml \\\n",
    "                 --img 640 640 \\\n",
    "                 --cfg cfg/training/yolov7_pothole.yaml \\\n",
    "                 --weights 'yolov7_training.pt' \\\n",
    "                 --name yolov7_pothole \\\n",
    "                 --hyp data/hyp.scratch.custom.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa7bf1-b316-44e0-8d1b-50e77c9505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/train/yolov7_pothole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96042a89-130c-49d6-90dd-2f0581dd7bca",
   "metadata": {},
   "source": [
    "## 4. Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85900a0b-d96a-484b-87c9-7ce565594e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --data data/pothole.yaml \\\n",
    "               --img 640 \\\n",
    "               --batch 32 \\\n",
    "               --conf 0.001 \\\n",
    "               --iou 0.65 \\\n",
    "               --device 0 \\\n",
    "               --weights runs/train/yolov7_pothole8/weights/best.pt \\\n",
    "               --name yolov7_640_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e384c6-61aa-42f2-83b6-af4c7c12ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/test/yolov7_640_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b799c2-0a02-4667-af88-da9d57f896ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='runs/test/yolov7_640_val/confusion_matrix.png',width=600,height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c57000-8cd5-4cf2-a5af-cad8312fde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='runs/test/yolov7_640_val/test_batch0_labels.jpg',width=1024,height=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b543c4-26cb-4a70-844b-9db8082b295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='runs/test/yolov7_640_val/test_batch1_labels.jpg',width=1024,height=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5596faf-067a-4788-92ed-cd0a7d6fe58d",
   "metadata": {},
   "source": [
    "## 5. Inferencia para prototipado rápido (PyTorch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3ff26-cb49-476c-a39d-dede623ee632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De https://github.com/pytorch/pytorch/issues/18325: torch.load() requires model module in the same folder #3678\n",
    "%cd yolov7\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e869cc7-eef0-4dab-820a-33e8824b2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/train/yolov7_pothole8/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa965b5c-7102-4474-a0e7-5e80fcebbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Atención: torch.load() requires model module in the same folder #3678\n",
    "MODEL_WEIGHTS_PATH=\"runs/train/yolov7_pothole8/weights/best.pt\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weigths = torch.load(MODEL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f5239-febb-4458-ab11-4fce75a1b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = weigths['model']\n",
    "model = model.half().to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a9bc0-58b6-4d92-a2df-c4b5cbe2f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "TEST_IMG_PATH='pothole_dataset/images/test/img-294_jpg.rf.a16953e9091e3eecfc338ed3044ef294.jpg'\n",
    "img = cv2.imread(TEST_IMG_PATH) \n",
    "img =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e08031-6492-40d8-8c65-81b112ec8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img.copy()\n",
    "image = transforms.ToTensor()(image)\n",
    "image = torch.tensor(np.array([image.numpy()]))\n",
    "image = image.to(device)\n",
    "image = image.half()\n",
    "with torch.no_grad():\n",
    "    output, _ = model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a1fc8-e235-43cd-8f8b-4a5c91c15149",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Despliegue con Triton Server\n",
    "\n",
    "Se siguen los pasos del [github oficial](https://github.com/WongKinYiu/yolov7/tree/main/deploy/triton-inference-server).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d49d64-b988-4e0b-8663-118a4b06f913",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Advertencia sobre compatibilidad entre TensorRT y CUDA**\n",
    "\n",
    "Elegir la versión de TensorRT que se corresponda con la versión de CUDA disponible en el host. De lo contrario aparecen errores.\n",
    "\n",
    "0. Determinar la versión de CUDA en el sistema.\n",
    "\n",
    "~~~bash\n",
    "nvidia-smi\n",
    "~~~\n",
    "\n",
    "1. Exportar a ONNX. \n",
    "\n",
    "Nota: instalar antes todas las dependencias:\n",
    "\n",
    "~~~bash\n",
    "pip install onnx onnx-simplifier onnx-graphsurgeon\n",
    "~~~\n",
    "\n",
    "~~~bash\n",
    "cd yolov7\n",
    "python export.py --weights runs/train/yolov7_pothole8/weights/best.pt --grid --end2end --dynamic-batch --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640\n",
    "~~~\n",
    "\n",
    "2. Exportar a TensorRT con docker de Triton. Elegir el que corresponda, por ejemplo, para CUDA 11.6 es 22.02.\n",
    "\n",
    "~~~bash\n",
    "docker run -it --rm --gpus=all nvcr.io/nvidia/tensorrt:22.02-py3\n",
    "~~~\n",
    "\n",
    "3. Copiar al docker.\n",
    "\n",
    "~~~bash\n",
    "docker ps\n",
    "CONTAINER ID   IMAGE                               COMMAND                  CREATED         STATUS         PORTS     NAMES\n",
    "14c431abcf03   nvcr.io/nvidia/tensorrt:22.06-py3   \"/opt/nvidia/nvidia_…\"   2 minutes ago   Up 2 minutes             dreamy_northcutt\n",
    "~~~\n",
    "\n",
    "~~~bash\n",
    "cd yolov7\n",
    "docker cp runs/train/yolov7_pothole8/weights/best.onnx dreamy_northcutt:/workspace/\n",
    "~~~\n",
    "\n",
    "4. Convertir a TensorRT\n",
    "\n",
    "~~~bash\n",
    "mv best.onnx yolov7.onnx\n",
    "./tensorrt/bin/trtexec --onnx=yolov7.onnx --minShapes=images:1x3x640x640 --optShapes=images:8x3x640x640 --maxShapes=images:8x3x640x640 --fp16 --workspace=4096 --saveEngine=yolov7-fp16-1x8x8.engine --timingCacheFile=timing.cache\n",
    "~~~\n",
    "\n",
    "5. Servicio con Triton\n",
    "\n",
    "Crear estructura de directorios:\n",
    "\n",
    "~~~bash\n",
    "cd yolo7_custom_dataset\n",
    "mv yolov7/yolov7-fp16-1x8x8.engine triton-deploy/models/yolov7/1/model.plan\n",
    "mkdir -pv triton-deploy/models/yolov7/1\n",
    "touch triton-deploy/models/yolov7/config.pbtxt\n",
    "~~~\n",
    "\n",
    "Editar `config.pbtxt`:\n",
    "\n",
    "~~~\n",
    "name: \"yolov7\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 8\n",
    "dynamic_batching { }\n",
    "~~~\n",
    "\n",
    "**Advertencia**: elegir la versión de Triton Server, se debe usar la misma versión de TensorRT que se usó para exportar. Ver [matriz de compatibilidad](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html)\n",
    "\n",
    "~~~bash\n",
    "docker run --gpus all --rm --ipc=host --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/models:/models nvcr.io/nvidia/tritonserver:22.02-py3 tritonserver --model-repository=/models --strict-model-config=false --log-verbose 1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a178323-e05f-436c-9566-70eeec01688e",
   "metadata": {},
   "source": [
    "### 6.1 Inferencia - Cliente gRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d3a80-cb0f-4597-919a-274a8451e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tritonclient[all] opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b3d782-0662-4388-94eb-2fd944482211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "TRITON_SERVER_URL='localhost:8001'\n",
    "\n",
    "# Create server context\n",
    "try:\n",
    "    triton_client = grpcclient.InferenceServerClient(\n",
    "        url=TRITON_SERVER_URL,\n",
    "        verbose=False,\n",
    "        ssl=False,\n",
    "        root_certificates=None,\n",
    "        private_key=None,\n",
    "        certificate_chain=None)\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef27f9b-c732-4c1c-ade4-a61f4a23a8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba33709c-771e-499a-ad6c-dea24c8e2757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2eeecd-9ea3-4214-a8ab-9d3d6d5d0a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8b8527-2433-456e-a22e-4053892ce920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRITON_MODEL_NAME=\"yolov7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f23591c-55a3-42a7-b015-67fbfed3d636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"yolov7\"\n",
       "versions: \"1\"\n",
       "platform: \"tensorrt_plan\"\n",
       "inputs {\n",
       "  name: \"images\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: -1\n",
       "  shape: 3\n",
       "  shape: 640\n",
       "  shape: 640\n",
       "}\n",
       "outputs {\n",
       "  name: \"num_dets\"\n",
       "  datatype: \"INT32\"\n",
       "  shape: -1\n",
       "  shape: 1\n",
       "}\n",
       "outputs {\n",
       "  name: \"det_boxes\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: -1\n",
       "  shape: 100\n",
       "  shape: 4\n",
       "}\n",
       "outputs {\n",
       "  name: \"det_scores\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: -1\n",
       "  shape: 100\n",
       "}\n",
       "outputs {\n",
       "  name: \"det_classes\"\n",
       "  datatype: \"INT32\"\n",
       "  shape: -1\n",
       "  shape: 100\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = triton_client.get_model_metadata(TRITON_MODEL_NAME)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659ed5ca-f1c5-402b-9541-101cb2c8c369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config {\n",
       "  name: \"yolov7\"\n",
       "  platform: \"tensorrt_plan\"\n",
       "  version_policy {\n",
       "    latest {\n",
       "      num_versions: 1\n",
       "    }\n",
       "  }\n",
       "  max_batch_size: 8\n",
       "  input {\n",
       "    name: \"images\"\n",
       "    data_type: TYPE_FP32\n",
       "    dims: 3\n",
       "    dims: 640\n",
       "    dims: 640\n",
       "  }\n",
       "  output {\n",
       "    name: \"num_dets\"\n",
       "    data_type: TYPE_INT32\n",
       "    dims: 1\n",
       "  }\n",
       "  output {\n",
       "    name: \"det_boxes\"\n",
       "    data_type: TYPE_FP32\n",
       "    dims: 100\n",
       "    dims: 4\n",
       "  }\n",
       "  output {\n",
       "    name: \"det_scores\"\n",
       "    data_type: TYPE_FP32\n",
       "    dims: 100\n",
       "  }\n",
       "  output {\n",
       "    name: \"det_classes\"\n",
       "    data_type: TYPE_INT32\n",
       "    dims: 100\n",
       "  }\n",
       "  instance_group {\n",
       "    name: \"yolov7\"\n",
       "    count: 1\n",
       "    gpus: 0\n",
       "    kind: KIND_GPU\n",
       "  }\n",
       "  default_model_filename: \"model.plan\"\n",
       "  dynamic_batching {\n",
       "    preferred_batch_size: 8\n",
       "  }\n",
       "  optimization {\n",
       "    input_pinned_memory {\n",
       "      enable: true\n",
       "    }\n",
       "    output_pinned_memory {\n",
       "      enable: true\n",
       "    }\n",
       "  }\n",
       "  backend: \"tensorrt\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = triton_client.get_model_config(TRITON_MODEL_NAME)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91dc1f53-3545-492e-84df-7ec17ae132c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb3c02b-d765-451a-863e-543850a23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = [\"images\"]\n",
    "OUTPUT_NAMES = [\"num_dets\", \"det_boxes\", \"det_scores\", \"det_classes\"]\n",
    "\n",
    "WIDTH=640\n",
    "HEIGHT=640\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "inputs.append(grpcclient.InferInput(INPUT_NAMES[0], [1, 3, WIDTH, HEIGHT], \"FP32\"))\n",
    "outputs.append(grpcclient.InferRequestedOutput(OUTPUT_NAMES[0]))\n",
    "outputs.append(grpcclient.InferRequestedOutput(OUTPUT_NAMES[1]))\n",
    "outputs.append(grpcclient.InferRequestedOutput(OUTPUT_NAMES[2]))\n",
    "outputs.append(grpcclient.InferRequestedOutput(OUTPUT_NAMES[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "148c791b-727a-47b3-9d2d-faf56f02b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "INPUT=\"yolov7/pothole_dataset/images/test/img-238_jpg.rf.f146df7999e374dbeaba65f92c518159.jpg\"\n",
    "input_image = cv2.imread(INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01afc802-c1a6-42f2-8aaf-8891789ade34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 720, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4828887-b97a-417a-8db2-97d5d1edb874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = cv2.resize(input_image, (640,640))\n",
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfd5afcb-04b0-491c-a74c-dd0144575b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    def __init__(self, classID, confidence, x1, x2, y1, y2, image_width, image_height):\n",
    "        self.classID = classID\n",
    "        self.confidence = confidence\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "        self.u1 = x1 / image_width\n",
    "        self.u2 = x2 / image_width\n",
    "        self.v1 = y1 / image_height\n",
    "        self.v2 = y2 / image_height\n",
    "\n",
    "    def box(self):\n",
    "        return (self.x1, self.y1, self.x2, self.y2)\n",
    "\n",
    "    def width(self):\n",
    "        return self.x2 - self.x1\n",
    "\n",
    "    def height(self):\n",
    "        return self.y2 - self.y1\n",
    "\n",
    "    def center_absolute(self):\n",
    "        return (0.5 * (self.x1 + self.x2), 0.5 * (self.y1 + self.y2))\n",
    "\n",
    "    def center_normalized(self):\n",
    "        return (0.5 * (self.u1 + self.u2), 0.5 * (self.v1 + self.v2))\n",
    "\n",
    "    def size_absolute(self):\n",
    "        return (self.x2 - self.x1, self.y2 - self.y1)\n",
    "\n",
    "    def size_normalized(self):\n",
    "        return (self.u2 - self.u1, self.v2 - self.v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba49f1f1-2a7f-4671-be1f-eb6fe26639b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, input_shape, letter_box=True):\n",
    "    if letter_box:\n",
    "        img_h, img_w, _ = img.shape\n",
    "        new_h, new_w = input_shape[0], input_shape[1]\n",
    "        offset_h, offset_w = 0, 0\n",
    "        if (new_w / img_w) <= (new_h / img_h):\n",
    "            new_h = int(img_h * new_w / img_w)\n",
    "            offset_h = (input_shape[0] - new_h) // 2\n",
    "        else:\n",
    "            new_w = int(img_w * new_h / img_h)\n",
    "            offset_w = (input_shape[1] - new_w) // 2\n",
    "        resized = cv2.resize(img, (new_w, new_h))\n",
    "        img = np.full((input_shape[0], input_shape[1], 3), 127, dtype=np.uint8)\n",
    "        img[offset_h:(offset_h + new_h), offset_w:(offset_w + new_w), :] = resized\n",
    "    else:\n",
    "        img = cv2.resize(img, (input_shape[1], input_shape[0]))\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2, 0, 1)).astype(np.float32)\n",
    "    img /= 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad5c89fe-ad9e-45b7-a0be-473acade0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(num_dets, det_boxes, det_scores, det_classes, img_w, img_h, input_shape, letter_box=True):\n",
    "    boxes = det_boxes[0, :num_dets[0][0]] / np.array([input_shape[0], input_shape[1], input_shape[0], input_shape[1]], dtype=np.float32)\n",
    "    scores = det_scores[0, :num_dets[0][0]]\n",
    "    classes = det_classes[0, :num_dets[0][0]].astype(np.int)\n",
    "\n",
    "    old_h, old_w = img_h, img_w\n",
    "    offset_h, offset_w = 0, 0\n",
    "    if letter_box:\n",
    "        if (img_w / input_shape[1]) >= (img_h / input_shape[0]):\n",
    "            old_h = int(input_shape[0] * img_w / input_shape[1])\n",
    "            offset_h = (old_h - img_h) // 2\n",
    "        else:\n",
    "            old_w = int(input_shape[1] * img_h / input_shape[0])\n",
    "            offset_w = (old_w - img_w) // 2\n",
    "\n",
    "    boxes = boxes * np.array([old_w, old_h, old_w, old_h], dtype=np.float32)\n",
    "    if letter_box:\n",
    "        boxes -= np.array([offset_w, offset_h, offset_w, offset_h], dtype=np.float32)\n",
    "    boxes = boxes.astype(np.int)\n",
    "\n",
    "    detected_objects = []\n",
    "    for box, score, label in zip(boxes, scores, classes):\n",
    "        detected_objects.append(BoundingBox(label, score, box[0], box[2], box[1], box[3], img_w, img_h))\n",
    "    return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db572b31-b3f5-4c78-b450-09f51e8cf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_buffer = preprocess(input_image, [WIDTH, HEIGHT])\n",
    "input_image_buffer = np.expand_dims(input_image_buffer, axis=0)\n",
    "\n",
    "inputs[0].set_data_from_numpy(input_image_buffer)\n",
    "\n",
    "results = triton_client.infer(model_name=TRITON_MODEL_NAME,\n",
    "                              inputs=inputs,\n",
    "                              outputs=outputs,\n",
    "                              client_timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5cd165b-8281-4a35-af8d-7a1a43a2964a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0279a7f7-420f-4505-83e4-368d12b65146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_stats {\n",
       "  name: \"yolov7\"\n",
       "  version: \"1\"\n",
       "  last_inference: 1663625727226\n",
       "  inference_count: 3\n",
       "  execution_count: 3\n",
       "  inference_stats {\n",
       "    success {\n",
       "      count: 3\n",
       "      ns: 1724882595\n",
       "    }\n",
       "    fail {\n",
       "    }\n",
       "    queue {\n",
       "      count: 3\n",
       "      ns: 436640\n",
       "    }\n",
       "    compute_input {\n",
       "      count: 3\n",
       "      ns: 37089336\n",
       "    }\n",
       "    compute_infer {\n",
       "      count: 3\n",
       "      ns: 1686269196\n",
       "    }\n",
       "    compute_output {\n",
       "      count: 3\n",
       "      ns: 606645\n",
       "    }\n",
       "    cache_hit {\n",
       "    }\n",
       "  }\n",
       "  batch_stats {\n",
       "    batch_size: 1\n",
       "    compute_input {\n",
       "      count: 3\n",
       "      ns: 37089336\n",
       "    }\n",
       "    compute_infer {\n",
       "      count: 3\n",
       "      ns: 1686269196\n",
       "    }\n",
       "    compute_output {\n",
       "      count: 3\n",
       "      ns: 606645\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = triton_client.get_inference_statistics(model_name=TRITON_MODEL_NAME)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ed53e9-3d42-45c8-8537-a82c469fe489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tritonclient.grpc.InferResult at 0x7f80547d5fa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25335398-08f4-4787-9268-7cf53264ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4685/95148301.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  classes = det_classes[0, :num_dets[0][0]].astype(np.int)\n",
      "/tmp/ipykernel_4685/95148301.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  boxes = boxes.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "num_dets = results.as_numpy(OUTPUT_NAMES[0])\n",
    "det_boxes = results.as_numpy(OUTPUT_NAMES[1])\n",
    "det_scores = results.as_numpy(OUTPUT_NAMES[2])\n",
    "det_classes = results.as_numpy(OUTPUT_NAMES[3])\n",
    "detected_objects = postprocess(num_dets, det_boxes, det_scores, det_classes, input_image.shape[1], input_image.shape[0], [WIDTH, HEIGHT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d7423db-e7e2-4f28-ab06-0568d19b9af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detected_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77cd98da-4384-4922-a27a-f9f2f3fbfd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 313, 485, 433)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_objects[0].box()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fe6fc-9b97-41f8-94a8-0873e1995f18",
   "metadata": {},
   "source": [
    "### 6.2 Inferencia - Integración en Videoanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50f6e369-1aa6-466e-a934-259d5456d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'videoanalytics'...\n",
      "remote: Enumerating objects: 824, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 824 (delta 3), reused 19 (delta 2), pack-reused 801\u001b[K\n",
      "Receiving objects: 100% (824/824), 12.24 MiB | 1.56 MiB/s, done.\n",
      "Resolving deltas: 100% (407/407), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nhorro/videoanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b41b7af-01c1-460a-b193-5620edf6f964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting networkx\n",
      "  Downloading networkx-2.8.6-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m470.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: networkx\n",
      "Successfully installed networkx-2.8.6\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd65393-ffb5-4723-877b-bb9faed39c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"videoanalytics/src/\")\n",
    "\n",
    "from videoanalytics.pipeline import Pipeline\n",
    "from videoanalytics.pipeline.sources import VideoReader\n",
    "from videoanalytics.pipeline.sinks import VideoWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d150e56-e167-47fe-a3b1-8bbb561dd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "from videoanalytics.utils.boundingboxes import format_boxes\n",
    "from videoanalytics.pipeline import Sink\n",
    "import numpy as np\n",
    "\n",
    "class YOLOv7DetectorTriton(Sink):\n",
    "    '''\n",
    "    YOLOv7 object detector Triton Client implementation.\n",
    "    This component **READS** the following entries in the global context:\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    | Variable name     | Description                                         |\n",
    "    +===================+============+==========+=============================+\n",
    "    | FRAME             | Numpy array representing the frame.                 |\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    This component **UPDATES** the following entries in the global context:\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    | Variable name     | Description                                         |\n",
    "    +===================+============+==========+=============================+\n",
    "    | DETECTIONS        | List holding numpy array with bounding boxes.       |\n",
    "    +-------------------+-----------------------------------------------------+\n",
    "    Args:\n",
    "        name(str): the component unique name.\n",
    "        context (dict): The global context.         \n",
    "        triton_server_uri (str): Triton server endpoint. \n",
    "        allowed_classes (list): set of allowed classes. This option is to restrict\n",
    "                                the detections to a subset of classes relevant to\n",
    "                                the application domain. If None, all classes are allowed. \n",
    "                                Note: Ignored in this version.\n",
    "        yolo_input_size (int): size in pixels of the input cell. The input image is \n",
    "                               resized using opencv. \n",
    "        yolo_max_output_size_per_class (int): maximum number of detections per class. \n",
    "                                              Note: Ignored in this version.\n",
    "        yolo_max_total_size (int): maximum number of detections. \n",
    "                                   Note: Ignored in this version.\n",
    "        context_name(str): variable name used for storing detections in context\n",
    "    '''    \n",
    "    def __init__(self,name,context, model_name=\"yolov7\", url='localhost:8001', context_name=\"DETECTIONS\"):\n",
    "        super().__init__(name, context)\n",
    "        \n",
    "        self.context_name=context_name\n",
    "        self.model_name=model_name\n",
    "        self.yolo_input_size=640\n",
    "        \n",
    "        self.letter_box=False # No soportado por ahora\n",
    "        \n",
    "        # Create server context        \n",
    "        self.triton_client = grpcclient.InferenceServerClient(\n",
    "            url=url,\n",
    "            verbose=False,\n",
    "            ssl=False,\n",
    "            root_certificates=None,\n",
    "            private_key=None,\n",
    "            certificate_chain=None)\n",
    "        \n",
    "        # Health check\n",
    "        assert(self.triton_client.is_server_live())\n",
    "        assert(self.triton_client.is_server_ready())\n",
    "        assert(self.triton_client.is_model_ready(self.model_name))\n",
    "        \n",
    "        self.INPUT_NAMES = [\"images\"]\n",
    "        self.OUTPUT_NAMES = [\"num_dets\", \"det_boxes\", \"det_scores\", \"det_classes\"]\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.inputs.append(grpcclient.InferInput(self.INPUT_NAMES[0], [1, 3, self.yolo_input_size, self.yolo_input_size], \"FP32\"))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[0]))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[1]))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[2]))\n",
    "        self.outputs.append(grpcclient.InferRequestedOutput(self.OUTPUT_NAMES[3]))\n",
    "        \n",
    "    def setup(self):\n",
    "        pass\n",
    "    \n",
    "    def __preprocess(self):\n",
    "        if self.letter_box:            \n",
    "            img_h, img_w, _ = self.context[\"FRAME\"].shape\n",
    "            new_h, new_w =   self.yolo_input_size,  self.yolo_input_size \n",
    "            offset_h, offset_w = 0, 0\n",
    "            if (new_w / img_w) <= (new_h / img_h):\n",
    "                new_h = int(img_h * new_w / img_w)\n",
    "                offset_h = (self.yolo_input_size - new_h) // 2\n",
    "            else:\n",
    "                new_w = int(img_w * new_h / img_h)\n",
    "                offset_w = (self.yolo_input_size - new_w) // 2\n",
    "            resized = cv2.resize(self.context[\"FRAME\"], (new_w, new_h))\n",
    "            img = np.full((self.yolo_input_size, self.yolo_input_size, 3), 127, dtype=np.uint8)\n",
    "            img[offset_h:(offset_h + new_h), offset_w:(offset_w + new_w), :] = resized\n",
    "        else:\n",
    "            img = cv2.resize(self.context[\"FRAME\"], (self.yolo_input_size, self.yolo_input_size))\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.transpose((2, 0, 1)).astype(np.float32)\n",
    "        img /= 255.0\n",
    "        return img\n",
    "            \n",
    "    def process(self):\n",
    "        input_image_buffer = self.__preprocess()\n",
    "        input_image_buffer = np.expand_dims(input_image_buffer, axis=0)\n",
    "        \n",
    "        self.inputs[0].set_data_from_numpy(input_image_buffer)\n",
    "        results = self.triton_client.infer(model_name=self.model_name,\n",
    "                                           inputs=self.inputs,\n",
    "                                           outputs=self.outputs)\n",
    "                \n",
    "        num_objects = results.as_numpy(self.OUTPUT_NAMES[0])[0][0]\n",
    "        bboxes = results.as_numpy(self.OUTPUT_NAMES[1])[0]\n",
    "        scores = np.squeeze(results.as_numpy(self.OUTPUT_NAMES[2]))        \n",
    "        classes = np.squeeze(results.as_numpy(self.OUTPUT_NAMES[3]))        \n",
    "        bboxes = bboxes[0:int(num_objects)].copy()       \n",
    "        scores = scores[0:int(num_objects)].copy()        \n",
    "        classes = classes[0:int(num_objects)].copy()\n",
    "        \n",
    "        \n",
    "                        \n",
    "        # 6. Convertir BBs de normalized ymin, xmin, ymax, xmax ---> xmin, ymin, width, height\n",
    "        original_h, original_w, _ = self.context[\"FRAME\"].shape\n",
    "        \n",
    "        for box in bboxes:\n",
    "            box[2]-=box[0]\n",
    "            box[3]-=box[1]\n",
    "            box[0]*=original_w/self.yolo_input_size\n",
    "            box[1]*=original_h/self.yolo_input_size\n",
    "            box[2]*=original_w/self.yolo_input_size\n",
    "            box[3]*=original_h/self.yolo_input_size\n",
    "        #bboxes = format_boxes(bboxes, original_h, original_w)\n",
    "        #print(bboxes)\n",
    "        \n",
    "        # 7. FIXME: encontrar una forma mejor de representar las detecciones\n",
    "        self.context[self.context_name] = [bboxes, scores, classes, num_objects]\n",
    "        \n",
    "    def shutdown(self):\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "32966916-0283-4fa5-8934-fd3bdd162bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009694337844848633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 4,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84b101faa5e45ec98dc9e95209752bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_avg_dt': 0.0004873317557927262, 'detector_avg_dt': 0.014562267780698643, 'annotator_avg_dt': 5.002672315050293e-06, 'writer_avg_dt': 0.0027257958645516243}\n"
     ]
    }
   ],
   "source": [
    "# Specific components for object detection\n",
    "from videoanalytics.pipeline.sinks.object_detection import DetectionsAnnotator, DetectionsCSVWriter\n",
    "\n",
    "# Input\n",
    "INPUT_VIDEO = \"test_video/video3.mp4\"\n",
    "OUTPUT_VIDEO = \"output3.mp4\"\n",
    "START_FRAME = 0\n",
    "MAX_FRAMES = None\n",
    "\n",
    "# Classes names for Detections Annotator\n",
    "DETECTOR_CLASSES_FILENAME = \"classes.txt\"\n",
    "\n",
    "# Output\n",
    "\n",
    "\n",
    "# 1. Create the context\n",
    "context = {}\n",
    "\n",
    "# 2. Create the pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# 3. Add components\n",
    "pipeline.add_component( VideoReader( \"input\",context,\n",
    "                                     video_path=INPUT_VIDEO,\n",
    "                                     start_frame=START_FRAME,\n",
    "                                     max_frames=MAX_FRAMES))\n",
    "\n",
    "# 3.2 Detector\n",
    "pipeline.add_component( YOLOv7DetectorTriton(\"detector\",context) )\n",
    "\n",
    "# 3.4 Annotate detections in output video\n",
    "pipeline.add_component( DetectionsAnnotator(\"annotator\",context,\n",
    "                                             class_names_filename=DETECTOR_CLASSES_FILENAME,\n",
    "                                             show_label=True) )\n",
    "\n",
    "pipeline.add_component(VideoWriter(\"writer\",context,filename=OUTPUT_VIDEO))\n",
    "\n",
    "# 4. Define connections\n",
    "pipeline.set_connections([\n",
    "    (\"input\", \"detector\"),\n",
    "    (\"detector\", \"annotator\"),\n",
    "    (\"annotator\", \"writer\")\n",
    "])\n",
    "                       \n",
    "# 5. Execute\n",
    "pipeline.execute()\n",
    "\n",
    "# 6. Report (optional)\n",
    "print(pipeline.get_metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
